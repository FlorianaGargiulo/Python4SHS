{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c197ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df46ec7",
   "metadata": {},
   "source": [
    "Téléchargez le fichier à l’URL:\n",
    "\n",
    "https://drive.google.com/file/d/110c7H7mjmfE_SwmgvD2iOmYzRSGDRF5C/view?usp=share_link\n",
    "\n",
    "C'est un echantillon de 500.000 posts sur Reddit, pris par:\n",
    "\n",
    "https://zenodo.org/records/4007913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa91af",
   "metadata": {},
   "source": [
    "#### Ex. 1 \n",
    "Ouvrir le fichier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a562b",
   "metadata": {},
   "source": [
    "#### Ex.2 \n",
    "Convertir le temps en datetime \n",
    "(nb. les dates sont en timestamps Unix mais ça ne change rien)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eda203",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a391e8",
   "metadata": {},
   "source": [
    "#### Ex. 3 \n",
    "Combien de subreddits sont présents dans le dataframe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9236df0",
   "metadata": {},
   "source": [
    "#### Ex. 4 \n",
    "Créer un dataframe qui résume l’activité de chaque subreddit.  \n",
    "Pour chaque subreddit, calculer :  \n",
    "- `nombre_de_posts` : le nombre total de posts publiés  \n",
    "- `nombre_d_auteurs` : le nombre d’auteurs différents ayant posté  \n",
    "- `premier_post` : la date du premier post  \n",
    "- `dernier_post` : la date du dernier post  \n",
    "\n",
    "Après traitement, le dataframe des statistiques des subreddits ressemblera :\n",
    "\n",
    "| subreddit       | nombre_de_posts | nombre_d_auteurs | premier_post        | dernier_post        |\n",
    "|----------------|----------------|-----------------|-------------------|-------------------|\n",
    "| depression      | 2              | 1               | 2019-01-01 00:00  | 2019-01-01 01:00  |\n",
    "| RedPillWives    | 2              | 1               | 2019-01-01 00:05  | 2019-01-01 03:00  |\n",
    "| Braincels       | 1              | 1               | 2019-01-01 02:00  | 2019-01-01 02:00  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa2f3f",
   "metadata": {},
   "source": [
    "#### Ex. 5 \n",
    "Quels sont les 50 premiers posts publiés ? Dans quels subreddits ont-ils été publiés ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788efeb",
   "metadata": {},
   "source": [
    "#### Ex6.\n",
    "Créer un dataframe qui résume l’activité de chaque auteur.  \n",
    "Pour chaque auteur, calculer :  \n",
    "- `nb_post` : le nombre total de posts publiés  \n",
    "- `nb_subreddits` : le nombre de subreddits différents dans lesquels il a posté  \n",
    "- `liste_subreddits` : la liste de subreddits différents dans lesquels il a posté  \n",
    "\n",
    "Après traitement, le dataframe des statistiques des auteurs ressemblera :\n",
    "\n",
    "| author    | nb_post | nb_subreddits |\n",
    "|-----------|---------|---------------|\n",
    "| alice123  | 3       | 2             |\n",
    "| bob456    | 2       | 1             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49ef8b",
   "metadata": {},
   "source": [
    "#### Ex. 7 \n",
    "1. Identifier les subreddits qui contiennent au moins 1000 posts dans le dataframe des statistiques des subreddits (exercice 4).  \n",
    "2. Filtrer le dataframe original pour ne garder que les posts appartenant à ces subreddits actifs.\n",
    "\n",
    "**Astuce :** pour filtrer le dataframe original, utiliser la forme :  \n",
    "`dfRed=df[df['subreddit'].isin(list_active_subreddits)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f1aee",
   "metadata": {},
   "source": [
    "#### Ex. 8: Mots les plus fréquents par subreddit\n",
    "\n",
    "**Objectif :** Trouver les 10 mots les plus utilisés dans chaque subreddit du dataframe filtré dans l'exercice precedente.  \n",
    "\n",
    "**Étapes :**  \n",
    "1. Regrouper tous les textes de chaque subreddit dans une liste de textes.  \n",
    "2. Fusionner les listes de textes en un texte unique par subreddit.  \n",
    "3. Appliquer la fonction de nettoyage du texte fournie sur ces textes.  \n",
    "4. Compter la fréquence des mots pour chaque subreddit.  \n",
    "5. Sélectionner les 10 mots les plus fréquents.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "528df078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def nettoyer_texte(text):\n",
    "    \"\"\"\n",
    "    Nettoie un texte :\n",
    "    - met en minuscules\n",
    "    - supprime la ponctuation et les chiffres\n",
    "    - enlève les stopwords NLTK\n",
    "    - supprime les mots trop courts\n",
    "    Retourne une liste de mots propres\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words and len(w) >= 2]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffed83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
